{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised sentiment analyses based on movie reviews\n",
    "## Based on 50,000 labeled IMDb movie reviews [dataset](http://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Python files with functions\n",
    "import text_normalizer as tn\n",
    "import model_evaluation_utils as meu\n",
    "# ~Python files\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('movie_reviews.csv')\n",
    "\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# extract data for model evaluation\n",
    "test_size = 500\n",
    "test_reviews = reviews[-test_size:]\n",
    "test_sentiments = sentiments[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some random reviews for future in-depth interpretation of the results of our models on them\n",
    "# sample_review_ids = [7626, 3533, 13010]\n",
    "sample_review_ids = [442,154,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize data\n",
    "norm_test_reviews = tn.normalize_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with AFINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/fnielsen/afinn/\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('') + 'afinn\\\\afinn\\\\')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from afinn import Afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "afn = Afinn(emoticons=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment for sample reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:  The movie is not that bad, Ringo Lam sucks. I hate when Van Damme has love in his movies, van Damme is good only when he doesn't have love in his movies.\n",
      "Actual sentiment:  negative\n",
      "Predicted sentiment polarity:  0.0\n",
      "------------------------------------------------------------\n",
      "Review:  This film is one of Michael Keaton's best. Throughout the film he is 'on'. With co-stars like Ms. Henner, Joe Piscopo and Danny DeVito, you can't go wrong. Great laughs, great fun for everyone.\n",
      "Actual sentiment:  positive\n",
      "Predicted sentiment polarity:  14.0\n",
      "------------------------------------------------------------\n",
      "Review:  A quite easy to watch tale of 2 thieves, with that love/hate type relationship between them. Chrisopher Walken stars and is very good as the silent rogue with a scam bigger than he's letting on.\n",
      "Actual sentiment:  positive\n",
      "Predicted sentiment polarity:  2.0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids],\n",
    "                             test_sentiments[sample_review_ids]):\n",
    "    print('Review: ', review)\n",
    "    print('Actual sentiment: ', sentiment)\n",
    "    print('Predicted sentiment polarity: ', afn.score(review))\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ the text used in a raw form because AFINN takes into account emoticons, exclamations etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_polarity = [afn.score(review) for review in test_reviews]\n",
    "predicted_sentiments = [\n",
    "    'positive' if score >= 1. else 'negative' for score in sentiment_polarity\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ the threshold should be chosen based on analyzing corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate AFINN model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.704\n",
      "Precision: 0.7127\n",
      "Recall: 0.704\n",
      "F1 Score: 0.7005\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.67      0.81      0.73       252\n",
      "   negative       0.76      0.60      0.67       248\n",
      "\n",
      "avg / total       0.71      0.70      0.70       500\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive        204       48\n",
      "        negative        100      148\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(\n",
    "    true_labels=test_sentiments,\n",
    "    predicted_labels=predicted_sentiments,\n",
    "    classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ many negative were classified as positive, fine-tuning the threshiold may be useful "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive polarity score:  0.875\n",
      "Negative polarity score:  0.125\n",
      "Objective score:  0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "awesome = list(swn.senti_synsets('awesome', 'a'))[0]\n",
    "print('Positive polarity score: ', awesome.pos_score())\n",
    "print('Negative polarity score: ', awesome.neg_score())\n",
    "print('Objective score: ', awesome.obj_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [senti_synsets description](https://stackoverflow.com/a/38263475)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build SentiWordNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment_sentiwordnet_lexicon(review, verbose=False):\n",
    "    # tokenize and POS tag text tokens\n",
    "    tagged_text = [(token.text, token.tag_) for token in tn.nlp(review)]\n",
    "\n",
    "    pos_score = neg_score = token_count = obj_score = 0.\n",
    "\n",
    "    # get word synsets based on POS tags\n",
    "    # get sentiment scores if synstets are found\n",
    "    for word, tag in tagged_text:\n",
    "        ss_set = None\n",
    "        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n",
    "        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n",
    "        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n",
    "        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n",
    "        # if synset is found\n",
    "        if ss_set:\n",
    "            # add scores for all found synsets\n",
    "            pos_score += ss_set.pos_score()\n",
    "            neg_score += ss_set.neg_score()\n",
    "            obj_score += ss_set.obj_score()\n",
    "            token_count += 1\n",
    "\n",
    "    # aggregate final scores\n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_statement = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "\n",
    "    if verbose:\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
    "\n",
    "        # display te results\n",
    "        df = pd.DataFrame(\n",
    "            [[\n",
    "                final_statement, norm_obj_score, norm_pos_score,\n",
    "                norm_neg_score, norm_final_score\n",
    "            ]],\n",
    "            columns=pd.MultiIndex(\n",
    "                levels=[['Sentiment stats'], [\n",
    "                    'Prediced sentiment', 'Objectivity', 'Positive',\n",
    "                    'Negative', 'Overall'\n",
    "                ]],\n",
    "                labels=[[0, 0, 0, 0, 0], [0, 1, 2, 3, 4]]))\n",
    "        print(df)\n",
    "\n",
    "    return final_statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment for sample reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:  movie not bad ringo lam suck hate van damme love movie van damme good not love movie\n",
      "Actual sentiment:  negative\n",
      "     Sentiment stats                                      \n",
      "  Prediced sentiment Objectivity Positive Negative Overall\n",
      "0           negative         0.7     0.14     0.16   -0.02\n",
      "------------------------------------------------------------\n",
      "Review:  film one michael keaton best throughout film co star like ms henner joe piscopo danny devito not go wrong great laugh great fun everyone\n",
      "Actual sentiment:  positive\n",
      "     Sentiment stats                                      \n",
      "  Prediced sentiment Objectivity Positive Negative Overall\n",
      "0           positive        0.86     0.09     0.05    0.04\n",
      "------------------------------------------------------------\n",
      "Review:  quite easy watch tale 2 thief love hate type relationship chrisopher walken star good silent rogue scam big let\n",
      "Actual sentiment:  positive\n",
      "     Sentiment stats                                      \n",
      "  Prediced sentiment Objectivity Positive Negative Overall\n",
      "0           positive        0.71     0.17     0.12    0.04\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip([norm_test_reviews[i] for i in sample_review_ids],\n",
    "                             test_sentiments[sample_review_ids]):\n",
    "    print('Review: ', review)\n",
    "    print('Actual sentiment: ', sentiment)\n",
    "    pred_sentiment = analyze_sentiment_sentiwordnet_lexicon(\n",
    "        review, verbose=True)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_sentiments = [\n",
    "    analyze_sentiment_sentiwordnet_lexicon(review)\n",
    "    for review in norm_test_reviews\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate SentiWordNet model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.67\n",
      "Precision: 0.6747\n",
      "Recall: 0.67\n",
      "F1 Score: 0.6673\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.65      0.76      0.70       252\n",
      "   negative       0.70      0.58      0.64       248\n",
      "\n",
      "avg / total       0.67      0.67      0.67       500\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive        191       61\n",
      "        negative        104      144\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(\n",
    "    true_labels=test_sentiments,\n",
    "    predicted_labels=predicted_sentiments,\n",
    "    classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build VADER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment_vader_model(review, threshold=0.1, verbose=False):\n",
    "    # pre-process text\n",
    "    review = tn.strip_html_tags(review)\n",
    "    review = tn.remove_accented_chars(review)\n",
    "    review = tn.expand_contractions(review)\n",
    "\n",
    "    # analyze the sentiment for review\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    # get aggregate scores and final sentiment\n",
    "    agg_score = scores['compound']\n",
    "    final_sentiment = 'positive' if agg_score >= threshold else 'negative'\n",
    "\n",
    "    if verbose:\n",
    "        # display detailed sentiment statistics\n",
    "        positive = str(round(scores['pos'], 2) * 100) + '%'\n",
    "        negative = str(round(scores['neg'], 2) * 100) + '%'\n",
    "        neutral = str(round(scores['neu'], 2) * 100) + '%'\n",
    "        final = round(agg_score, 2)\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            [[final_sentiment, final, positive, negative, neutral]],\n",
    "            columns=pd.MultiIndex(\n",
    "                levels=[['Sentiment stats:'], [\n",
    "                    'Predicted sentiment', 'Polarity score', 'Positive',\n",
    "                    'Negative', 'Neutral'\n",
    "                ]],\n",
    "                labels=[[0, 0, 0, 0, 0], [0, 1, 2, 3, 4]]))\n",
    "        print(df)\n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ during preprocessing punctuation and emoticons were kept intact\n",
    "### Recommendations for threshold value\n",
    "+ for positive > 0.5\n",
    "+ for neutral [-0.5, 0.5]\n",
    "+ for negative < -0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment for sample reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:  The movie is not that bad, Ringo Lam sucks. I hate when Van Damme has love in his movies, van Damme is good only when he doesn't have love in his movies.\n",
      "Actual sentiment:  negative\n",
      "     Sentiment stats:                                                     \n",
      "  Predicted sentiment Polarity score Positive Negative             Neutral\n",
      "0            positive           0.84    30.0%    13.0%  56.00000000000001%\n",
      "Review:  This film is one of Michael Keaton's best. Throughout the film he is 'on'. With co-stars like Ms. Henner, Joe Piscopo and Danny DeVito, you can't go wrong. Great laughs, great fun for everyone.\n",
      "Actual sentiment:  positive\n",
      "     Sentiment stats:                                                    \n",
      "  Predicted sentiment Polarity score Positive            Negative Neutral\n",
      "0            positive           0.95    40.0%  7.000000000000001%   54.0%\n",
      "Review:  A quite easy to watch tale of 2 thieves, with that love/hate type relationship between them. Chrisopher Walken stars and is very good as the silent rogue with a scam bigger than he's letting on.\n",
      "Actual sentiment:  positive\n",
      "     Sentiment stats:                                         \n",
      "  Predicted sentiment Polarity score Positive Negative Neutral\n",
      "0            negative          -0.16    15.0%    17.0%   68.0%\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('Review: ',review)\n",
    "    print('Actual sentiment: ',sentiment)\n",
    "    pred_sentiment = analyze_sentiment_vader_model(review,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_sentiments = [analyze_sentiment_vader_model(review) for review in test_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate VADER model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.698\n",
      "Precision: 0.7068\n",
      "Recall: 0.698\n",
      "F1 Score: 0.6943\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.67      0.81      0.73       252\n",
      "   negative       0.75      0.59      0.66       248\n",
      "\n",
      "avg / total       0.71      0.70      0.69       500\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive        203       49\n",
      "        negative        102      146\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(\n",
    "    true_labels=test_sentiments,\n",
    "    predicted_labels=predicted_sentiments,\n",
    "    classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As the result the best model is AFINN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
